<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>CSL7640 Assignment 1 - Problem 4</title>

<style>
body {
    font-family: Arial, sans-serif;
    margin: 50px;
    line-height: 1.6;
    background-color: #f9f9f9;
}

h1, h2, h3, h4 {
    color: #2c3e50;
}

hr {
    margin: 30px 0;
}

table {
    border-collapse: collapse;
    width: 80%;
    margin-bottom: 20px;
}

table, th, td {
    border: 1px solid #444;
}

th, td {
    padding: 8px;
    text-align: center;
}

th {
    background-color: #eaeaea;
}

code {
    background-color: #eee;
    padding: 3px 6px;
    border-radius: 4px;
}

ul {
    margin-bottom: 20px;
}

.section {
    margin-bottom: 40px;
}
</style>

</head>
<body>

<h1>CSL7640 — Assignment 1, Problem 4</h1>
<h2>Sports vs Politics Document Classifier</h2>

<p><strong>Student:</strong> Sahilpreet Singh (B23CS1061)<br>
<strong>Course:</strong> CSL7640 — Natural Language Understanding, IIT Jodhpur<br>
<strong>Date:</strong> February 2026</p>

<hr>

<div class="section">
<h2>1. Introduction</h2>

<p>This report presents a text document classification system that categorizes documents into two classes: <strong>Sports</strong> and <strong>Politics</strong>. The task is to design, implement, and compare at least three machine learning techniques using different feature representations including n-grams, TF-IDF, and Bag of Words.</p>

<p>The motivation behind this task is to understand how classical machine learning techniques perform on a fundamental NLP problem — document classification.</p>
</div>

<hr>

<div class="section">
<h2>2. Data Collection and Dataset Description</h2>

<h3>2.1 Data Collection Method</h3>
<p>The dataset was manually curated to ensure high quality and diversity. Each sentence represents a typical news headline belonging to either Sports or Politics.</p>

<h3>2.2 Dataset Statistics</h3>

<table>
<tr><th>Property</th><th>Value</th></tr>
<tr><td>Total samples</td><td>100</td></tr>
<tr><td>Sports samples</td><td>50</td></tr>
<tr><td>Politics samples</td><td>50</td></tr>
<tr><td>Class balance</td><td>50:50</td></tr>
<tr><td>Average sentence length</td><td>10–12 words</td></tr>
<tr><td>Train-test split</td><td>80:20</td></tr>
</table>

<h3>2.3 Dataset Analysis</h3>

<p><strong>Sports sub-topics:</strong></p>
<ul>
<li>Cricket, Football, Tennis</li>
<li>Basketball, Olympics</li>
<li>Formula 1, Boxing, Badminton, Hockey</li>
</ul>

<p><strong>Politics sub-topics:</strong></p>
<ul>
<li>Elections and Parliament</li>
<li>Policy and legislation</li>
<li>Diplomacy and governance</li>
<li>Defense and economic policy</li>
</ul>

<h3>2.4 Vocabulary Analysis</h3>
<p>
Unigram vocabulary size: <strong>388</strong><br>
Unigram + Bigram vocabulary size: <strong>964</strong>
</p>

</div>

<hr>

<div class="section">
<h2>3. Feature Representations</h2>

<h3>3.1 Bag of Words (BoW)</h3>
<p><code>BoW(d, w) = count of w in d</code></p>

<p><strong>Advantages:</strong></p>
<ul>
<li>Simple</li>
<li>Works well for small datasets</li>
</ul>

<p><strong>Disadvantages:</strong></p>
<ul>
<li>Ignores word order</li>
<li>Common words dominate</li>
</ul>

<h3>3.2 TF-IDF</h3>

<p>
<code>TF(w,d) = count(w in d) / total_words(d)</code><br>
<code>IDF(w) = log((N+1)/(df(w)+1)) + 1</code>
</p>

<p><strong>Advantages:</strong></p>
<ul>
<li>Down-weights common words</li>
<li>Highlights discriminative words</li>
</ul>

<h3>3.3 N-gram Features</h3>

<p>Captures word sequences like <code>"prime minister"</code>.</p>

<p><strong>Disadvantages:</strong></p>
<ul>
<li>Larger feature space</li>
<li>Higher sparsity</li>
</ul>

</div>

<hr>

<div class="section">
<h2>4. Machine Learning Techniques</h2>

<h3>4.1 Multinomial Naive Bayes</h3>

<p>
<code>P(class | document) ∝ P(class) × ∏ P(word | class)</code>
</p>

<p>
<code>P(w | c) = (count(w,c)+1)/(total_words(c)+|V|)</code>
</p>

<h3>4.2 Logistic Regression</h3>

<p>
<code>P(y=1 | x) = 1 / (1 + exp(-(w·x + b)))</code>
</p>

<h3>4.3 K-Nearest Neighbors</h3>

<p>
<code>sim(a,b) = (a · b) / (||a|| ||b||)</code>
</p>

</div>

<hr>

<div class="section">
<h2>5. Results</h2>

<h3>Naive Bayes + BoW</h3>

<table>
<tr><th>Metric</th><th>SPORTS</th><th>POLITICS</th></tr>
<tr><td>Precision</td><td>1.0000</td><td>0.8182</td></tr>
<tr><td>Recall</td><td>0.8182</td><td>1.0000</td></tr>
<tr><td>F1</td><td>0.9000</td><td>0.9000</td></tr>
<tr><td><strong>Accuracy</strong></td><td colspan="2"><strong>90%</strong></td></tr>
</table>

<h3>KNN + Bigram</h3>

<table>
<tr><th>Metric</th><th>SPORTS</th><th>POLITICS</th></tr>
<tr><td>Precision</td><td>1.0000</td><td>0.6923</td></tr>
<tr><td>Recall</td><td>0.6364</td><td>1.0000</td></tr>
<tr><td>F1</td><td>0.7778</td><td>0.8182</td></tr>
<tr><td><strong>Accuracy</strong></td><td colspan="2"><strong>80%</strong></td></tr>
</table>

</div>

<hr>

<div class="section">
<h2>6. Limitations</h2>
<ul>
<li>Small dataset size</li>
<li>No cross-validation</li>
<li>Surface-level features only</li>
<li>Binary classification only</li>
<li>Manual dataset may lack real-world complexity</li>
</ul>
</div>

<hr>

<div class="section">
<h2>7. Conclusion</h2>
<p>
Naive Bayes and Logistic Regression achieved 90% accuracy, while KNN achieved 80%. Feature representation plays a crucial role in performance.
</p>
</div>

<hr>

<div class="section">
<h2>8. References</h2>
<ol>
<li>Manning et al. (2008). Introduction to Information Retrieval.</li>
<li>Jurafsky & Martin (2023). Speech and Language Processing.</li>
<li>Sebastiani (2002). Machine learning in text categorization.</li>
</ol>
</div>

</body>
</html>
